{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Installing required libraries**"
      ],
      "metadata": {
        "id": "n9yfXJxajv61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "bm-ZEXRSX1-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the BERT Tokenizer**"
      ],
      "metadata": {
        "id": "x6ZLW3tukIMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4tx5zI2V7cf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import the dataset and reframe it as per the goal of the project**"
      ],
      "metadata": {
        "id": "LAfvkViIj04M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTsNXxB4bQ5w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('data.json')\n",
        "\n",
        "dataset = {'title':[], 'paragraph': [], 'context': [], 'question': [], 'answer': [], 'answer_start': []}\n",
        "for i in range(df['data'].shape[0]):\n",
        "  paragraph_count = 0\n",
        "  for j in range(len(df['data'][i]['paragraphs'])):\n",
        "    for k in range(len(df['data'][i]['paragraphs'][j]['qas'])):\n",
        "      for l in range(len(df['data'][i]['paragraphs'][j]['qas'][k]['answers'])):\n",
        "        dataset['title'].append(df['data'][i]['title'])\n",
        "        dataset['paragraph'].append(paragraph_count)\n",
        "        dataset['context'].append(df['data'][i]['paragraphs'][j]['context'])\n",
        "        dataset['question'].append(df['data'][i]['paragraphs'][j]['qas'][k]['question'])\n",
        "        dataset['answer'].append(df['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text'])\n",
        "        dataset['answer_start'].append(df['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['answer_start'])\n",
        "    paragraph_count += 1\n",
        "\n",
        "data = pd.DataFrame.from_dict(dataset)\n",
        "\n",
        "train = []\n",
        "for i in range(len(dataset['question'])):\n",
        "    train.append({\n",
        "        'context': dataset['context'][i],\n",
        "        'question': dataset['question'][i],\n",
        "        'answer': {\n",
        "            'text': dataset['answer'][i],\n",
        "            'answer_start': dataset['answer_start'][i]}\n",
        "        })\n",
        "\n",
        "train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Definition to pre-process the training data**"
      ],
      "metadata": {
        "id": "LwphC22TkTB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co5dzB-bT8Fw"
      },
      "outputs": [],
      "source": [
        "# We need the input as follows ==> [<START> \"Question here...\" <SEP> \"Context here...\" <END>]\n",
        "# And the output should be like ==> [{start-of-answer: <INT>, end-of-answer: <INT>}]\n",
        "\n",
        "def preprocess(list_items):\n",
        "  # Tokenizing the question and context\n",
        "  # Pointers:\n",
        "  # 1. max_length: To restrict the \"question + context\" length to 384\n",
        "  # 2. truncation: To truncate from the \"context\" part off the token if length exceeds 384\n",
        "  # 3. return_offsets_mapping: offset_mapping is ==> String: \"This is a sentence\" --> Token: [<START>, 0, 1, 2, 3, <END>] --> offset_list: [(0, 0), (0, 4), (5, 7), (8, 9), (10, 17), (18, 18)]\n",
        "  inputs = tokenizer(\n",
        "        list_items['question'],\n",
        "        list_items[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "  # The code below is to store the start and end positions of the answers\n",
        "  ##############################################################################\n",
        "  sequence_ids = inputs.sequence_ids(0)\n",
        "\n",
        "  offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "  answer = list_items[\"answer\"]\n",
        "  start_positions = 0\n",
        "  end_positions = 0\n",
        "\n",
        "  start_char = answer[\"answer_start\"]\n",
        "  end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
        "\n",
        "  # Find the start and end of the context\n",
        "  idx = 0\n",
        "  while sequence_ids[idx] != 1:\n",
        "    idx += 1\n",
        "  context_start = idx\n",
        "  while sequence_ids[idx] == 1:\n",
        "    idx += 1\n",
        "  context_end = idx - 1\n",
        "\n",
        "  # If the answer is not fully inside the context, label it (0, 0)\n",
        "  if offset_mapping[context_start][0] > end_char or offset_mapping[context_end][1] < start_char:\n",
        "    start_positions = 0\n",
        "    end_positions = 0\n",
        "  else:\n",
        "    # Otherwise it's the start and end token positions\n",
        "    idx = context_start\n",
        "    while idx <= context_end and offset_mapping[idx][0] <= start_char:\n",
        "        idx += 1\n",
        "    start_positions = (idx - 1)\n",
        "\n",
        "    idx = context_end\n",
        "    while idx >= context_start and offset_mapping[idx][1] >= end_char:\n",
        "      idx -= 1\n",
        "    end_positions = (idx + 1)\n",
        "\n",
        "  inputs[\"start_positions\"] = start_positions\n",
        "  inputs[\"end_positions\"] = end_positions\n",
        "  ##############################################################################\n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Using 'map' to apply the definition made above to every training data**"
      ],
      "metadata": {
        "id": "IHiyvuzek40U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcEW_aH1WWJX"
      },
      "outputs": [],
      "source": [
        "tokenized_items = list(map(preprocess, train))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the 'model' and Preparing the training data**"
      ],
      "metadata": {
        "id": "fKyTGm1wmKNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "EMBgmtY9XQls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(tokenized_items)\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_train_set"
      ],
      "metadata": {
        "id": "1rcvDclvmQzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating the optimizer before training**"
      ],
      "metadata": {
        "id": "rwAq-JFgmWAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 2\n",
        "total_train_steps = (len(tokenized_items) // batch_size) * num_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=total_train_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "5G3_XxN7XHKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Compiling the model created**"
      ],
      "metadata": {
        "id": "lW4o-F2vmb9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model.compile(optimizer=optimizer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5_L5gXsMdwNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training**"
      ],
      "metadata": {
        "id": "nSl33hMbmhBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=tf_train_set, epochs=2, batch_size = 1024)"
      ],
      "metadata": {
        "id": "DiRaj4etdfja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Trying to use numpy arrays instead of tf.datasets (Throws error)**"
      ],
      "metadata": {
        "id": "3xF_mrQ3xidY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Whenever I try using this, I get an error as below:\n",
        "# ValueError: Cannot reshape a tensor with 768 elements to shape [1,1,384,1] (384 elements) for '{{node tf_bert_for_sequence_classification_3/bert/embeddings/LayerNorm/Reshape}} =\n",
        "# Reshape[T=DT_FLOAT, Tshape=DT_INT32](tf_bert_for_sequence_classification_3/bert/embeddings/LayerNorm/Reshape/ReadVariableOp,\n",
        "# tf_bert_for_sequence_classification_3/bert/embeddings/LayerNorm/Reshape/shape)' with input shapes: [768], [4] and with input tensors computed as partial shapes: input1 = [1,1,384,1].\n",
        "\n",
        "'''import numpy as np\n",
        "X = np.array([\n",
        "  np.array(\n",
        "    [i['input_ids'] for i in tokenized_items]\n",
        "    ),\n",
        "  np.array(\n",
        "    [i['attention_mask'] for i in tokenized_items]\n",
        "    )])\n",
        "X\n",
        "\n",
        "Y = np.array([\n",
        "  np.array(\n",
        "    [i['start_positions'] for i in tokenized_items]\n",
        "    ),\n",
        "  np.array(\n",
        "    [i['end_positions'] for i in tokenized_items]\n",
        "    )])\n",
        "Y\n",
        "model.fit(x=X, y=Y, epochs=2, batch_size = 1024)'''"
      ],
      "metadata": {
        "id": "Ik5OrlyqtSxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Validating the output**"
      ],
      "metadata": {
        "id": "6Y_CyRvGmmCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"How many programming languages does BLOOM support?\"\n",
        "question2 = \"How many parameters does BLOOM have?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
        "\n",
        "inputs = tokenizer(question1, context, return_tensors=\"tf\")\n",
        "outputs = model(**inputs)\n",
        "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
        "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "print(tokenizer.decode(predict_answer_tokens))\n",
        "\n",
        "inputs = tokenizer(question2, context, return_tensors=\"tf\")\n",
        "outputs = model(**inputs)\n",
        "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
        "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "print(tokenizer.decode(predict_answer_tokens))"
      ],
      "metadata": {
        "id": "do-CZKRURrNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SitypljSnNoa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}