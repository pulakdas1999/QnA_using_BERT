{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Installing required libraries**"
      ],
      "metadata": {
        "id": "mADEMYVvbY3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cps2N6X4KOM"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the BERT Tokenizer**"
      ],
      "metadata": {
        "id": "6xxVzvNmbfpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "ktEcQFgY1bOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Loading the in-built squad-dataset from tf.datasets**"
      ],
      "metadata": {
        "id": "856si2gCbm4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmVkvJ1u8uOX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
        "\n",
        "squad = squad.train_test_split(test_size=0.2)\n",
        "squad[\"train\"][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Definition to pre-process the training data**"
      ],
      "metadata": {
        "id": "lwx98O1ub8JO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URUL_mhU9G7R"
      },
      "outputs": [],
      "source": [
        "# We need the input as follows ==> [<START> \"Question here...\" <SEP> \"Context here...\" <END>]\n",
        "# And the output should be like ==> [{start-of-answer: <INT>, end-of-answer: <INT>}]\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenizing the question and context\n",
        "    # Pointers:\n",
        "    # 1. max_length: To restrict the \"question + context\" length to 384\n",
        "    # 2. truncation: To truncate from the \"context\" part off the token if length exceeds 384\n",
        "    # 3. return_offsets_mapping: offset_mapping is ==> String: \"This is a sentence\" --> Token: [<START>, 0, 1, 2, 3, <END>] --> offset_list: [(0, 0), (0, 4), (5, 7), (8, 9), (10, 17), (18, 18)]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # The code below is to store the start and end positions of the answers\n",
        "    ############################################################################\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    ############################################################################\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Using 'map' to apply the definition made above to every training data**"
      ],
      "metadata": {
        "id": "nZW3f8BqgKMA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnYCvG_-vSe"
      },
      "outputs": [],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the 'model' and Preparing the training and validation data**"
      ],
      "metadata": {
        "id": "6BcKEjYhgmpT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXceFthnCIlZ"
      },
      "outputs": [],
      "source": [
        "# Importing the model from HuggingFace Transformers module\n",
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_squad[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = model.prepare_tf_dataset(\n",
        "    tokenized_squad[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_train_set"
      ],
      "metadata": {
        "id": "WZLAM1YsgllR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating the optimizer before training**"
      ],
      "metadata": {
        "id": "ZMRwvh-FhEf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au8WQx1WCFYB"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 2\n",
        "total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=total_train_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Compiling the model created**"
      ],
      "metadata": {
        "id": "v1kce22MhwdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT0r8q91CQs2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training**"
      ],
      "metadata": {
        "id": "-faIxiv-h2Ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nNC9Vl2CTp_"
      },
      "outputs": [],
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Validating the output**"
      ],
      "metadata": {
        "id": "HZO0w0ZQh5Xc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WHFrs1bCXR8"
      },
      "outputs": [],
      "source": [
        "question = \"How many programming languages does BLOOM support?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
        "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
        "outputs = model(**inputs)\n",
        "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
        "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OqALhpaeNR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}